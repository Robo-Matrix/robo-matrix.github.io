<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RoboMatrix demo.">
  <meta name="keywords" content="LLM, VLLM, VLA, Embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_rm.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World</h1>
          <div class="is-size-5 publication-authors">
            <div style="display: inline-block; margin-right: 5px;">
              <a href="https://scholar.google.com/citations?user=6-bYm5EAAAAJ">Weixin Mao</a><sup>1,3</sup><span class="author-star">*</span>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Weiheng Zhong</a><sup>2</sup><span class="author-star">*</span>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Zhou Jiang</a><sup>2</sup>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Dong Fang</a><sup>4</sup>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Zhongyue Zhang</a><sup>2</sup>
            </div>
            <br>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Zihan Lan</a><sup>3</sup>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="#">Fan Jia</a><sup>3</sup>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="https://scholar.google.com/citations?user=YI0sRroAAAAJ">Tiancai Wang</a><sup>3</sup>
            </div>
            <div style="display: inline-block; margin-right: 5px;">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=bzzBut4AAAAJ">Haoqiang Fan</a><sup>3</sup>
            </div>
            <div style="display: inline-block;">
              <a href="https://www.waseda.jp/fsci/gips/other-en/2015/09/08/2160/">Osamu Yoshie</a><sup>1</sup>
            </div>
          </div>

          <!-- Author footnote -->
          <div class="is-size-5 publication-authors">
            <div>
                <span class="author-block"><sup>1</sup>Waseda University,</span>
                <span class="author-block"><sup>2</sup>Beijing Institute of Technology,</span>
            </div>
            <div>
                <span class="author-block"><sup>3</sup>Megvii Technology,</span>
                <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong</span>
            </div>
        </div>
        <div class="is-size-5 publication-authors">
            <p><strong>*</strong>Equal contribution</p>
        </div>
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- <a href="https://arxiv.org/pdf/2011.12948" -->
                <a href="https://arxiv.org/abs/2412.00171"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.00171"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/WayneMao/RoboMatrix/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/WayneMao/RoboMatrix"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-close_drawer">
          <video poster="" id="close_drawer" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/close_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-cross">
          <video poster="" id="cross" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/cross.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-grasp_object">
          <video poster="" id="grasp_object" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/grasp_object.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-move_to_object">
          <video poster="" id="move_to_object" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/move_to_object.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-move_to_place">
          <video poster="" id="move_to_place" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/move_to_place.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-open_drawer">
          <video poster="" id="open_drawer" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/open_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-place_object">
          <video poster="" id="place_object" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/place_object.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-release_object">
          <video poster="" id="release_object" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/skills/release_object.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing policy learning methods predominantly adopt the task-centric paradigm, 
            necessitating the collection of task data in an end-to-end manner. Consequently, 
            The learned policy tends to fail in tackling novel tasks. Moreover, 
            it is hard to localize the errors for a complex task with multiple stages due to end-to-end learning.
            To address these challenges, we propose <b>RoboMatrix</b>, a skill-centric and hierarchical framework 
            for scalable task planning and execution. We first introduce a novel skill-centric paradigm thatextracts 
            the common meta-skills from different complex tasks. This allows for the capture of embodied 
            demonstrations through a skill-centric approach, enabling the completion of open-world tasks 
            by combining learned meta-skills. 
            To fully leverage meta-skills, we further develop a hierarchical framework that decouples complex robot 
            tasks into three interconnected layers: (1) a high-level modular scheduling layer; (2) a middle-level 
            skill layer; and (3) a low-level hardware layer.
            Experimental results illustrate that our skill-centric and hierarchical framework achieves remarkable 
            generalization performance across novel objects, scenes, tasks, and embodiments. This framework offers 
            a novel solution for robot task planning and execution in open-world scenarios. Our software and hardware will 
            be made avaliable as open-source resources.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video controls autoplay muted loop playsinline width="100%">
            <source src="./static/videos/skills/place_object.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div> -->
    
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Skill-Centric</h2>
      
      <p class="content has-text-justified">
        <strong>Inspiration of the skill-centric method.</strong>
    Robots with different modalities can perform different tasks and robots with the same modality can be used in various scenarios. We extract similar elements from the multitude of diverse robotic tasks, defining these elements as meta-skills, and store them in a skill list. Then, these skills are used to train the Vision-Language-Action (VLA) model or to construct traditional models, which can eventually lead to a skill model capable of adapting to new tasks.
      </p>
      <br>
      <div style="text-align: center;">
        <img src="static/images/meta-skill.png" class="interpolation-image" style="width: 100%; height: auto;"/>
      </div>
    </div>

  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">RoboMatrix</h2>
      
      <p class="content has-text-justified">
        The system accepts the task description in either text or audio format. The text can be entered manually, while the audio is converted into text format by the audio-to-text module.
    The <strong>Modular Scheduling Layer</strong> serves as the high-level planner of the system. The agent decomposes complex tasks into an ordered sequence of subtasks based on the robot's skill list and adds them sequentially to the execution queue.
    Before executing a subtask, the execution checker verifies its executability by determining whether the object to be manipulated or grasped is present in the scene based on the robot's environment observations.
    The <strong>Skill Layer</strong> maps the description of subtasks to robot actions using either hybrid model or VLA model, with the action including a stop signal to determine whether the current subtask is complete.
    The <strong>Hardware Layer</strong> manages the controller and stage observer of the robot, with the controller converting actions into control signals and the stage observer continuously updating the robot's state and image in real-time.
      </p>
      <br>
      <div style="text-align: center;">
        <img src="static/images/framework.png" class="interpolation-image" style="width: 100%; height: auto;"/>
      </div>
    </div>

  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Generalization</h2>
      
      <p class="content has-text-justified">
        Vision-Language-Action model generalization to novel and challenging scenarios.
      </p>
      
      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop
            width="99%">
            <source src="./static/videos/move_pick_place_new_scene.mp4" 
            type="video/mp4">
          </video>
          <p style="text-align:center">
            "Place the pink cube into black toolbox."
          </p>
        </div>
    
        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop
            width="99%">
            <source src="./static/videos/crossing_obstacles.mp4" 
            type="video/mp4">
          </video>
          <p style="text-align:center">
            "Crossing the obstacles at the front."
          </p>
        </div>
  </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src=""
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src=""
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src=""
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src=""
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- New Scene1. -->
        <!-- <h3 class="title is-4">Scene Generalization</h3>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action model generalization to novel and challenging scenarios.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/move_pick_place_new_scene.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/crossing_obstacles.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ New Scene1. -->

        <!-- Audio-pepeline. -->
        <h3 class="title is-4">Dynamic Adversarial Interaction</h3>
        <div class="content has-text-justified">
          <p>
          A human dynamically manipulates the placement of obstacles in the robot's path. 
          The VLA model dynamically processes visual input and generates actions to help the robot avoid these obstacles.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/crossing_obstacles_with_adversarial_interaction.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ 15 steps. -->

        <!-- Audio-pepeline. -->
        <h3 class="title is-4">Super Long-horizon Tasks</h3>
        <div class="content has-text-justified">
          <p>
            RoboMatrix takes a composite task prompt as input, decomposes it into a skill list with <b>15 sequential steps</b>, 
            and leverages the VLA skill model to successfully execute the super long-Horizon tasks.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/clean_floor.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ 15 steps. -->

        <!-- Audio-pepeline. -->
        <!-- <h3 class="title is-4">Audio Input Pipeline</h3>
        <div class="content has-text-justified">
          <p>
            Voice input task description and complete pipeline demonstration of the RoboMatrix framework.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/audio_pipeline.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Audio-pepeline. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{mao2024robomatrixskillcentrichierarchicalframework,
      title={RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World}, 
      author={Weixin Mao and Weiheng Zhong and Zhou Jiang and Dong Fang and Zhongyue Zhang and Zihan Lan and Fan Jia and Tiancai Wang and Haoqiang Fan and Osamu Yoshie},
      year={2024},
      eprint={2412.00171},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2412.00171}, 
}
    </code></pre>
  </div>
</section>
<strong>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://voxposer.github.io/">VoxPose</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
